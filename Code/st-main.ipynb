{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from models import model\n",
    "from read_data import read_data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los archivos raw\n",
    "folder = os.path.join('..', 'Date')\n",
    "dataframes1 = read_data.read_raw(folder)\n",
    "folder = os.path.join('..', 'Date2')\n",
    "dataframes2 = read_data.read_raw(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Se procesaron 297 series de longitud 99\n",
      "[+] Se procesaron 32 series de longitud 99\n"
     ]
    }
   ],
   "source": [
    "# Convertimos todo en arrays de numpy con series del mismo largo\n",
    "data_1 = read_data.read_and_perform(dataframes1, row_range=300, col_range=(3,12), split= True)\n",
    "data_2 = read_data.read_and_perform(dataframes2, row_range=99, col_range=(2,5), split= False)\n",
    "# concatenamos todas las series\n",
    "data_total = np.vstack([data_1.T, data_2.T])\n",
    "\n",
    "# separamos train y test\n",
    "train, test = read_data.train_test_split(data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] StandardScaler entrenado\n",
      "[+] Train shape (310, 99)\n",
      "[+] Test shape (19, 99)\n"
     ]
    }
   ],
   "source": [
    "# escalamos\n",
    "scaler = read_data.scalings(train)\n",
    "train = scaler.fit_transform(train)\n",
    "print(f'[+] Train shape {train.shape}')\n",
    "test = scaler.transform(test)\n",
    "print(f'[+] Test shape {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar modelo para un solo espacio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el directorio para guardar las imágenes si no existe\n",
    "path_img = os.path.join(\"..\",\"img\",\"Drop-Optimo\")\n",
    "os.makedirs(path_img, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss = 1.0040003061294556\n",
      "100 loss = 0.11730137467384338\n",
      "200 loss = 0.08359379321336746\n",
      "300 loss = 0.04032127559185028\n",
      "400 loss = 0.033532217144966125\n",
      "500 loss = 0.028023280203342438\n",
      "600 loss = 0.02669672854244709\n",
      "700 loss = 0.02513948269188404\n",
      "800 loss = 0.020755019038915634\n",
      "900 loss = 0.020684024319052696\n",
      "1000 loss = 0.017455104738473892\n",
      "1100 loss = 0.01727764494717121\n",
      "1200 loss = 0.01715472899377346\n",
      "1300 loss = 0.014518149197101593\n",
      "1400 loss = 0.01809506118297577\n",
      "0 loss = 1.0044764280319214\n",
      "100 loss = 0.12969592213630676\n",
      "200 loss = 0.10075286030769348\n",
      "300 loss = 0.0756211131811142\n",
      "400 loss = 0.04765097424387932\n",
      "500 loss = 0.045192066580057144\n",
      "600 loss = 0.04274669662117958\n",
      "700 loss = 0.03829871863126755\n",
      "800 loss = 0.036373257637023926\n",
      "900 loss = 0.0296781063079834\n",
      "1000 loss = 0.029788149520754814\n",
      "1100 loss = 0.028872331604361534\n",
      "1200 loss = 0.02516286075115204\n",
      "1300 loss = 0.025525685399770737\n",
      "1400 loss = 0.02508639171719551\n",
      "0 loss = 1.0044333934783936\n",
      "100 loss = 0.13895368576049805\n",
      "200 loss = 0.11956637352705002\n",
      "300 loss = 0.07169666141271591\n",
      "400 loss = 0.059428196400403976\n",
      "500 loss = 0.051479849964380264\n",
      "600 loss = 0.04573880508542061\n",
      "700 loss = 0.045276939868927\n",
      "800 loss = 0.04375585541129112\n",
      "900 loss = 0.038083989173173904\n",
      "1000 loss = 0.03846358135342598\n",
      "1100 loss = 0.03664255887269974\n",
      "1200 loss = 0.03507200628519058\n",
      "1300 loss = 0.032007668167352676\n",
      "1400 loss = 0.0319560244679451\n",
      "0 loss = 1.0049022436141968\n",
      "100 loss = 0.1453988403081894\n",
      "200 loss = 0.12738947570323944\n",
      "300 loss = 0.12036404758691788\n",
      "400 loss = 0.07348023355007172\n",
      "500 loss = 0.07418128848075867\n",
      "600 loss = 0.06088118627667427\n",
      "700 loss = 0.06259143352508545\n",
      "800 loss = 0.055069416761398315\n",
      "900 loss = 0.048990413546562195\n",
      "1000 loss = 0.05188402906060219\n",
      "1100 loss = 0.053553637117147446\n",
      "1200 loss = 0.04419272020459175\n",
      "1300 loss = 0.040025025606155396\n",
      "1400 loss = 0.040637481957674026\n",
      "0 loss = 1.0046653747558594\n",
      "100 loss = 0.18536779284477234\n",
      "200 loss = 0.14328981935977936\n",
      "300 loss = 0.13112469017505646\n",
      "400 loss = 0.10493895411491394\n",
      "500 loss = 0.09392677992582321\n",
      "600 loss = 0.08072083443403244\n",
      "700 loss = 0.07066375017166138\n",
      "800 loss = 0.0676020011305809\n",
      "900 loss = 0.06679091602563858\n",
      "1000 loss = 0.06824749708175659\n",
      "1100 loss = 0.05444743111729622\n",
      "1200 loss = 0.054775990545749664\n",
      "1300 loss = 0.055027976632118225\n",
      "1400 loss = 0.052201710641384125\n",
      "0 loss = 1.0052735805511475\n",
      "100 loss = 0.2064872533082962\n",
      "200 loss = 0.17331922054290771\n",
      "300 loss = 0.15052755177021027\n",
      "400 loss = 0.14809758961200714\n",
      "500 loss = 0.13975125551223755\n",
      "600 loss = 0.13029912114143372\n",
      "700 loss = 0.1333276927471161\n",
      "800 loss = 0.12836113572120667\n",
      "900 loss = 0.13241268694400787\n",
      "1000 loss = 0.12337002158164978\n",
      "1100 loss = 0.11893066018819809\n",
      "1200 loss = 0.11547785997390747\n",
      "1300 loss = 0.10076946020126343\n",
      "1400 loss = 0.11219948530197144\n"
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "lr = 1e-3\n",
    "lat = 5\n",
    "drop = np.linspace(0.1,0.6,6)\n",
    "\n",
    "for dr in drop:\n",
    "    # Entrenamiento\n",
    "    hist_train = []\n",
    "    hist_test = []\n",
    "    autoencoder = model.NNAutoencoder(99, lat, dr)\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr = lr)\n",
    "    criterio = torch.nn.MSELoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        autoencoder.train()\n",
    "        x = torch.FloatTensor(train)\n",
    "        y_pred = autoencoder(x)\n",
    "        loss = criterio(y_pred, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if e%100 == 0:\n",
    "            print(e, \"loss =\",loss.item())\n",
    "        hist_train.append(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            autoencoder.eval()\n",
    "            x = torch.FloatTensor(test)\n",
    "            y_pred = autoencoder(x)\n",
    "            loss = criterio(y_pred, x)\n",
    "            hist_test.append(loss.item())\n",
    "\n",
    "    #guardo las img\n",
    "    plt.semilogy(hist_train, label = 'train loss')\n",
    "    plt.semilogy(hist_test, label = 'test loss')\n",
    "    plt.title(f\"Loss train -eval, drop = {dr}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(path_img,f\"lat{lat}-drop{dr}.png\"), bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Es  una técnica que permite detener el entrenamiento cuando el valor de pérdida en el conjunto de validación (test) comienza a aumentar. Esto ayuda a prevenir el sobreajuste y permite guardar el mejor modelo basado en el mínimo valor de pérdida en el conjunto de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el directorio para guardar las imágenes si no existe\n",
    "path_model = os.path.join(\"..\",\"Save-Models\")\n",
    "os.makedirs(path_model, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train Loss: 1.0036, test Loss: 1.1227\n",
      "Epoch 100, train Loss: 0.1295, test Loss: 0.0787\n",
      "Epoch 200, train Loss: 0.0679, test Loss: 0.0394\n",
      "Epoch 300, train Loss: 0.0479, test Loss: 0.0314\n",
      "Epoch 400, train Loss: 0.0445, test Loss: 0.0354\n",
      "Epoch 500, train Loss: 0.0396, test Loss: 0.0400\n",
      "Epoch 600, train Loss: 0.0363, test Loss: 0.0379\n",
      "Epoch 700, train Loss: 0.0321, test Loss: 0.0291\n",
      "Epoch 800, train Loss: 0.0312, test Loss: 0.0381\n",
      "Epoch 900, train Loss: 0.0288, test Loss: 0.0290\n",
      "Epoch 1000, train Loss: 0.0290, test Loss: 0.0245\n",
      "Epoch 1100, train Loss: 0.0268, test Loss: 0.0206\n",
      "Epoch 1200, train Loss: 0.0257, test Loss: 0.0250\n",
      "Epoch 1300, train Loss: 0.0243, test Loss: 0.0200\n",
      "Epoch 1400, train Loss: 0.0271, test Loss: 0.0199\n",
      "Best epoch was 279 with val loss 0.0246\n"
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "lr = 1e-3\n",
    "dr = 0.2\n",
    "lat = 5\n",
    "\n",
    "# Entrenamiento\n",
    "hist_train = []\n",
    "hist_test = []\n",
    "best_test_loss = 100\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "espera = 100\n",
    "b = 0 #bandera\n",
    "\n",
    "autoencoder = model.NNAutoencoder(99, lat, dr)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr = lr)\n",
    "criterio = torch.nn.MSELoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    autoencoder.train()\n",
    "    x = torch.FloatTensor(train)\n",
    "    y_pred = autoencoder(x)\n",
    "    loss = criterio(y_pred, x)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    hist_train.append(loss.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        autoencoder.eval()\n",
    "        x = torch.FloatTensor(test)\n",
    "        y_pred = autoencoder(x)\n",
    "        test_loss = criterio(y_pred, x)\n",
    "        hist_test.append(test_loss.item())\n",
    "\n",
    "    if e%100 == 0:\n",
    "            print(f'Epoch {e}, train Loss: {loss.item():.4f}, test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "    if test_loss < loss:\n",
    "        #guardo el mejor modelo\n",
    "        if test_loss < best_test_loss: #encuantra el primer minimo??\n",
    "            best_test_loss = test_loss\n",
    "            best_epoch = e\n",
    "            best_model = autoencoder.state_dict().copy() #copia del mejor modelo\n",
    "\n",
    "        if (e - best_epoch >= espera) and (b < 2):\n",
    "            m_epoch = best_epoch\n",
    "            m_test_loss = best_test_loss\n",
    "            torch.save(best_model, os.path.join(path_model,f\"model-lat{lat}.pth\"))\n",
    "            b += 1\n",
    "            plt.plot(m_epoch,m_test_loss,'x', color = \"red\")\n",
    "\n",
    "print(f'Best epoch was {m_epoch} with val loss {m_test_loss:.4f}')\n",
    "#guardo las img\n",
    "plt.semilogy(hist_train, label = 'train loss')\n",
    "plt.semilogy(hist_test, label = 'test loss')\n",
    "plt.plot(m_epoch,m_test_loss,'x', color = \"red\", label = \"best model\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(f\"dim_lat: {lat}, drop = {dr}\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(path_img,f\"BestModel-lat{lat}-drop{dr}.png\"), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAES_list = []\n",
    "for i in range(500):\n",
    "    autoencoder.train()\n",
    "    x = torch.FloatTensor(test)\n",
    "    y_pred = autoencoder(x)\n",
    "    sample = scaler.inverse_transform(x)\n",
    "    sample_pred = scaler.inverse_transform(y_pred.detach().numpy())\n",
    "\n",
    "    average_MAES = np.abs(sample-sample_pred).mean()\n",
    "    MAES_list.append(average_MAES)\n",
    "    print(f'DIM= {i} & Average MAE= {average_MAES:.3f}')\n",
    "\n",
    "array = np.array(MAES_list)\n",
    "mean = np.mean(array)\n",
    "std = np.std(array) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curva\n",
    "# Crear el histograma\n",
    "#plt.figure(figsize=(10, 6))\n",
    "sns.histplot(MAES_list, kde = True, bins=50, edgecolor='black')\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "plt.xlabel('MAES')\n",
    "plt.ylabel('f')\n",
    "plt.title('Frecuencia de ocurrencia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo MAE vs Dim Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.array(range(1,21)) #Espacio latente del 1 al 20\n",
    "MAES_list = [] #se guardaran ls MAES de cada espacio latente\n",
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "drop = 0.0005\n",
    "\n",
    "for lat in latent:\n",
    "    # Modelo\n",
    "    autoencoder = model.NNAutoencoder(99, lat, drop)\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr = lr)\n",
    "    criterio = torch.nn.MSELoss()\n",
    "    # Entrenamiento\n",
    "    hist_train = []\n",
    "    hist_test = []\n",
    "    for e in range(epochs):\n",
    "        autoencoder.train()\n",
    "        x = torch.FloatTensor(train)\n",
    "        y_pred = autoencoder(x)\n",
    "        loss = criterio(y_pred, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluacion\n",
    "    with torch.no_grad():\n",
    "        autoencoder.eval()\n",
    "        x = torch.FloatTensor(test)\n",
    "        y_pred = autoencoder(x)\n",
    "        sample = scaler.inverse_transform(x)\n",
    "        sample_pred = scaler.inverse_transform(y_pred.numpy())\n",
    "\n",
    "    average_MAES = np.abs(sample-sample_pred).mean()\n",
    "    MAES_list.append(average_MAES)\n",
    "    print(f'DIM= {lat} & Average MAE= {average_MAES:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "ax.errorbar(latent,MAES_list,0.05,fmt='o', linewidth=2, capsize=6)\n",
    "ax.plot(latent,MAES_list)\n",
    "ax.set_ylabel(\"MAES\")\n",
    "ax.set_xlabel(\"Laten Dim\")\n",
    "ax.set_xticks(np.arange(0, 22, 1))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(MAES_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el histograma\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y, bins=50, edgecolor='black')\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "plt.xlabel('MAES')\n",
    "plt.ylabel('f')\n",
    "plt.title('Frecuencia de ocurrencia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
